# Revis√£o Data Lake üèûÔ∏è
Reposit√≥rio para revisitar os conceitos principais de Data Lake, estudando para a prova de Engenheiro de Dados.

Baseado no [reposit√≥rio do professor](https://github.com/jlsilva01/adls-azure) e outros recursos oferecidos em sala.

## Table of Contents

- [Pr√©-requisitos](#üìö-pr√©-requisitos)
- [Passos](#üë£-passos)


## üìö Pr√©-requisitos

- Docker
- SQL Server Management Studio (SSMS)
- Python/pip, PyEnv, Pipx, Poetry (use o [guia do professor](https://storage.satc.edu.br/arquivos/docentes/4906/20241/files/ED/Python%20ED/Python%20para%20Engenharia%20de%20Dados.pdf) para uma explica√ß√£o completa da instala√ß√µes)
- Azure CLI
- Visual Studio Code _(ou qualquer editor)_
- Terraform
- Conta da Microsoft Learning (utilizando o Active Sandbox)
- Conta no [Databricks Community](https://community.databricks.com/)

## üë£ Passos

##### 1. Configura√ß√£o do Banco de Dados

###### 1.1 Criar um cont√¢iner no Docker com SQL Server
```bash
docker run -e "ACCEPT_EULA=Y" -e "MSSQL_SA_PASSWORD=satc@2023" -p 1433:1433 --name satc-sql-server --hostname satc-sql-server -d mcr.microsoft.com/mssql/server:2022-latest
```

###### 1.2 Conectar no SSMS 

```markdown
# Credenciais

nome do servidor: localhost
usu√°rio:          sa
senha:            satc@2023
```

##### 1.3 Execute o script de cria√ß√£o do banco

Dentro do SSMS, execute o arquivo `create_cars_database.sql` na pasta `scripts`.

##### 2. Ative uma assinatura de teste
[MS Learn Sandbox (√Årea Restrita)](https://learn.microsoft.com/pt-br/training/modules/develop-test-deploy-azure-functions-with-core-tools/5-exercise-publish-function-core-tools?ns-enrollment-type=learningpath&ns-enrollment-id=learn.create-serverless-applications) - Concierge Subscription _(4 horas de limite)_ 

##### 3. Azure CLI

###### 3.1 Login

```bash  copy
az login
```

###### 3.2 Consultar o nome do Resource Group criado para a sua conta do Concierge Subscription

```bash copy
az group list -o table
```

##### 4. Ajustar o arquivo `variables.tf` com o resultado do passo anterior

```terraform
variable "resource_group_name" {
  default = "learn-9b3e07c7-1725-4464-9b6a-b0fee4cb4c0a"
}
```

##### 5. Criar recursos na Azure
###### 5.1 - Terraform init

```bash
terraform init
```

###### 5.2. Validar c√≥digo em arquivos com `.tf`

```bash
terraform validate
```

###### 5.3. Ajustar formata√ß√£o em arquivos com `.tf`

```bash
terraform fmt
```

###### 5.4. Cria um plano de execu√ß√£o

```bash 
terraform plan
```

###### 5.5. Aplicar o Terraform na n√∫vem

```bash
terraform apply
```

##### 6. Checar o deploy do ADLS

Entrar em [portal.azure.com](https://portal.azure.com/) e checar o ADLS criado.

##### 7. Destruir recursos criados

**‚ö†Ô∏è Cuidado:** Use apenas se voc√™ n√£o quiser continuar com as pr√≥ximas etapas ‚ö†Ô∏è

```bash
terraform destroy
```

> <b>Nota:</b> Para usar `apply` e `destroy` sem confirma√ß√£o, use a tag `-auto-approve` (use pelo seu pr√≥prio risco).

##### 8. Env

###### 8.1. Copiar arquivo env
```bash
# Windows
copy .env.example .env

# Linux
cp .env.example .env
```

###### 8.2. Preencha as seguintes vari√°veis no `.env`:


`ADLS_ACCOUNT_NAME=datalakebf6f21398b44ea7b`

> Dispon√≠vel em https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Storage%2FStorageAccounts

`ADLS_SAS_TOKEN=sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupyx&se=2024-05-10T03:35:53Z&st=2024-05-09T19:35:53Z&spr=https&sig=SPucZ0LhY4PWAiIyzIhATo5sydM5WPyph%2Bl2javSi9k%3D`

> Dentro da conta de armazenamento selecionada anteriormente, basta acessar "Assinatura de acesso compartilhado", preencha todos os campos em "Tipos de recurso permitidos" e clique em **Gerar a cadeia de conex√£o e de SAS**. <br><br> Copie o **Token SAS** criado e cole nessa vari√°vel.

##### 9. Python

###### 9.1 Instalar depend√™ncias

```bash
cd python

poetry install
```

###### 9.2 Execute o script para N tabelas

```bash
poetry run python elt_sql_n_tabelas.py
```

##### 10. Databricks

###### 10.1 Crie um novo cluster

Dentro de **Compute**, clique em **Create compute**. Digite o nome do seu novo cluster (n√£o precisa de mais configura√ß√µes).

###### 10.2 Importar os notebooks

Dentro de **Workspace**, clique em **Home**. Clique no √≠cone de flecha, e selecione **Import**. 

Individualmente, importe todos os arquivos com `.dbc` dentro da pasta `databricks`.

###### 10.3 Execute os notebooks

Execute os notebooks. O dado deve navegar entre as camadas Bronze, Prata e Ouro, come√ßando na Landing-zone e aplicando todas as transforma√ß√µes para cada passo.